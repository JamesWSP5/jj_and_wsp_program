{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "spark = SparkSession.builder \\\n",
    "   .master(\"local\") \\\n",
    "   .appName(\"Word Count\") \\\n",
    "   .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "   .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "builder \n",
    "\n",
    "具有Builder构造SparkSession实例的class属性\n",
    "类Builder\n",
    "的构建器SparkSession\n",
    "\n",
    "\n",
    "appName（名称）\n",
    "设置应用程序的名称，该名称将显示在Spark Web UI中。\n",
    "\n",
    "如果未设置应用程序名称，将使用随机生成的名称。\n",
    "\n",
    "\n",
    "参量\n",
    "名称 –应用程序名称\n",
    "\n",
    "config（key = None，value = None，conf = None ）\n",
    "\n",
    "设置配置选项。使用此方法设置的选项会自动传播到SparkConf和SparkSession自己的配置中。\n",
    "\n",
    "enableHiveSupport（）\n",
    "\n",
    "启用Hive支持，包括与持久性Hive Metastore的连接，对Hive Serdes的支持以及Hive用户定义的功能。\n",
    "\n",
    "getOrCreate（）\n",
    "\n",
    "获取一个现有的，SparkSession或者如果不存在，则根据此构建器中设置的选项创建一个新的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CreateDataFrame（data，schema = None，sampleRatio = None，verifySchema = True ）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame从RDD，列表或创建一个pandas.DataFrame。\n",
    "\n",
    "\n",
    "\n",
    "当schema是列名称列表时，将从中推断出各列的类型data。\n",
    "\n",
    "\n",
    "当schema为is时None，它将尝试从中推断出模式（列名称和类型）data，该模式应为Row或namedtuple或的RDD dict。\n",
    "\n",
    "\n",
    "当schema为is pyspark.sql.types.DataType或数据类型字符串时，它必须与实际数据匹配，否则将在运行时引发异常。如果给定的模式不是 pyspark.sql.types.StructType，则将其包装为 pyspark.sql.types.StructType唯一字段，并且字段名将为“ value”，每条记录也将包装为元组，随后可将其转换为行。\n",
    "\n",
    "\n",
    "如果需要模式推断，samplingRatio则用于确定用于模式推断的行的比率。如果samplingRatio为，将使用第一行None。\n",
    "\n",
    "参量\n",
    "\n",
    "数据 –任何类型的SQL数据表示形式（例如，行，元组，整数，布尔值等）的RDD，或list或pandas.DataFrame。\n",
    "\n",
    "schema –一个pyspark.sql.types.DataType或数据类型字符串或列名列表，默认为None。数据类型的字符串格式等于 pyspark.sql.types.DataType.simpleString，除了顶级结构类型可以省略struct<>和原子类型typeName()用作其格式，例如，使用use byte代替tinyintfor pyspark.sql.types.ByteType。我们还可以将 int用作简称IntegerType。\n",
    "\n",
    "sampleRatio –用于推断的行的采样率\n",
    "\n",
    "\n",
    "verifySchema –根据模式验证每一行的数据类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1='Alice', _2=1)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " l = [('Alice', 1)]\n",
    " spark.createDataFrame(l).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Alice', age=1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " spark.createDataFrame(l, ['name', 'age']).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = [{'name': 'Alice', 'age': 1}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py:346: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(age=1, name='Alice')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(d).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1='Alice', _2=1)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "rdd = sc.parallelize(l)\n",
    "spark.createDataFrame(rdd).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Alice', age=1)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame(rdd, ['name', 'age'])\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Alice', age=1)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "Person = Row('name', 'age')\n",
    "person = rdd.map(lambda r: Person(*r))\n",
    "df2 = spark.createDataFrame(person)\n",
    "df2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Alice', age=1)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)])\n",
    "df3 = spark.createDataFrame(rdd, schema)\n",
    "df3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Alice', age=1)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(df.toPandas()).collect()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "newSession（）\n",
    "返回一个新的SparkSession作为新会话，该会话具有单独的SQLConf，已注册的临时视图和UDF，但是共享了SparkContext和表缓存。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "range（start，end = None，step = 1，numPartitions = None ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1), Row(id=3), Row(id=5)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.range(1, 7, 2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0), Row(id=1), Row(id=2)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.range(3).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.createOrReplaceTempView(\"table1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.sql(\"SELECT name AS f1, age as f2 from table1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(f1='Alice', f2=1)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "strlen = spark.udf.register(\"stringLengthString\", lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(stringLengthString(test)='4')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT stringLengthString('test')\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(stringLengthString(text)='3')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT 'foo' AS text\").select(strlen(\"text\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(stringLengthInt(test)=4)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "_ = spark.udf.register(\"stringLengthInt\", lambda x: len(x), IntegerType())\n",
    "spark.sql(\"SELECT stringLengthInt('test')\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(slen(test)=4)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "slen=udf(lambda x:len(x),IntegerType())\n",
    "_=spark.udf.register(\"slen\",slen)\n",
    "spark.sql(\"SELECT slen('test')\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(random_udf()=95)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random_udf = udf(lambda: random.randint(0, 100), IntegerType())\n",
    "new_random_udf = spark.udf.register(\"random_udf\", random_udf)\n",
    "spark.sql(\"SELECT random_udf()\").collect()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(add_one(id)='1'), Row(add_one(id)='2'), Row(add_one(id)='3')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "#     pandas_udf(\"integer\", PandasUDFType.SCALAR)  \n",
    "def add_one(x):\n",
    "     return x + 1\n",
    "_ = spark.udf.register(\"add_one\", add_one)  \n",
    "spark.sql(\"SELECT add_one(id) FROM range(3)\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(age)=1)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.agg({\"age\": \"max\"}).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(min(age)=1)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "df.agg(F.min(df.age)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-21\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "today=datetime.date.today()\n",
    "print(today)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Alice', name='Alice', age=1)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df_as1 = df.alias(\"df_as1\")\n",
    "df_as2 = df.alias(\"df_as2\")\n",
    "joined_df = df_as1.join(df_as2, col(\"df_as1.name\") == col(\"df_as2.name\"), 'left')\n",
    "joined_df.select(\"df_as1.name\", \"df_as2.name\", \"df_as2.age\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
